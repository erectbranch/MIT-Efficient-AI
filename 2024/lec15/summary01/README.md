# 15 Long-Context LLM

> [EfficientML.ai Lecture 15 - Long-Context LLM (MIT 6.5940, Fall 2024, Zoom Recording)](https://youtu.be/D3NlVsFod8w)

LLMì—ì„œ long-contextë¥¼ ì´í•´í•˜ë„ë¡ ë¯¸ì„¸ì¡°ì •í•˜ë ¤ë©´, êµ‰ì¥íˆ í° í•™ìŠµ ë¹„ìš©ì´ í•„ìš”í•˜ë‹¤.

---

## 15.1 LongLoRA

> [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models ë…¼ë¬¸(2023)](https://arxiv.org/abs/2309.12307)

ì˜ˆë¥¼ ë“¤ì–´ 8192 context lengthë¡œ LLMì„ ë¯¸ì„¸ì¡°ì •í•˜ë ¤ë©´, 2048 ëŒ€ë¹„ self-attention ë ˆì´ì–´ì—ì„œ 16ë°°ì˜ ì—°ì‚° ë¹„ìš©ì´ í•„ìš”í•˜ë‹¤.

LongLoRA ë…¼ë¬¸ì€ ë¯¸ì„¸ì¡°ì • ì‹œ sparse local attention( $S^2$ -Attn )ë¥¼ ì±„íƒí•˜ëŠ” ê²ƒìœ¼ë¡œ í•™ìŠµ ë¹„ìš©ì„ ìµœì í™”í•œë‹¤. (ì¶”ë¡ ì—ì„œëŠ” dense global attention ì‚¬ìš©)

---

### 15.1.1 Shifted Sparse Attention

> long-context ëª¨ë¸ì˜ ë³‘ëª©ì€ attention ì—°ì‚°ì´ë‹¤. (token lengthì— ë”°ë¼ quadraticí•˜ê²Œ ê³„ì‚° ë³µì¡ë„ê°€ ìƒìŠ¹í•˜ê¸° ë•Œë¬¸)

$S^2$ -Attnì€ ì‚¼ê°í˜• ì˜ì—­ì˜ íŠ¹ì • ê·¸ë£¹ë§Œì„ ì—°ì‚°í•˜ëŠ” sparse attention ê¸°ë²•ì´ë‹¤. 

- head ì ˆë°˜ì€ Pattern 1, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ Pattern 2ë¥¼ ë”°ë¥¸ë‹¤. (information flowê°€ ê·¸ë£¹ ê°„ êµí™˜ë˜ë„ë¡)

- Pattern 2 = group size ì ˆë°˜ ë§Œí¼ Pattern 1 shift

ë‘˜ì„ ì´í›„ ê²°í•©í•˜ëŠ” ê²ƒìœ¼ë¡œ information flowë¥¼ ìœ ì§€í•œë‹¤.

![LongLoRA overview 1](images/LongLoRA_overview_1.png)

ë‹¤ìŒì€ ë‹¤ì–‘í•œ context length ì¡°ê±´ì—ì„œ $S^2$ -Attnì˜ ì„±ëŠ¥ì„ ê²€ì¦í•œ ë„í‘œì´ë‹¤.

![LongLoRA S2 Attn](images/LongLoRA_s2attn_result.png)

> Llama2 7B, RedPajama ë°ì´í„°ì…‹, perplexity: PG19 ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ì¸¡ì •

---

### 15.1.2 Finetuning Normalization and Embedding

ê¸°ì¡´ full fine-tuning LoRAì™€ ì„±ëŠ¥ ê²©ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ, **Input Embedding**, **Normalization** ë‘ ë ˆì´ì–´ë¥¼ í•¨ê»˜ ë¯¸ì„¸ì¡°ì •í•œë‹¤.

![LongLoRA overview 2](images/LongLoRA_overview_2.png)

> ğŸ”¥: ë¯¸ì„¸ì¡°ì • ëŒ€ìƒ ë ˆì´ì–´

Llama2 7B ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ëŠ” ë¶ˆê³¼ 0.004%ë¥¼ ì°¨ì§€í•œë‹¤. ê·¸ëŸ¬ë‚˜ Input Embeddingê³¼ í•¨ê»˜ ë¯¸ì„¸ì¡°ì •í•˜ëŠ” ê²ƒìœ¼ë¡œ full FTì™€ì˜ ê²©ì°¨ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

![LongLoRa Enhanced](images/LongLoRA_finetune_layer.png)

> Llama2 7B, RedPajama ë°ì´í„°ì…‹, 32758 target context length, perplexity: PG19 ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ì¸¡ì •

---

### 15.1.3 LongLoRA: Results

ë‹¤ìŒì€ ë‹¤ì–‘í•œ context length ì„¤ì •ì—ì„œ perplexityì™€ í•™ìŠµ ì‹œê°„ì„ ì¸¡ì •í•œ ê²°ê³¼ë‹¤. perplexityëŠ” ê¸°ì¡´ Full FT(íŒŒë€ìƒ‰)ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ íšë“í•˜ë©´ì„œ, í•™ìŠµ ì‹œê°„ì€ ìµœëŒ€ 1.8x ë‹¨ì¶•í•˜ì˜€ë‹¤.

![LongLoRA result](images/LongLoRA_results.png)

> Llama2-7B, Flash-Attention-2, proof-pile í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹

ë˜í•œ, 32768 context length ì„¤ì •ìœ¼ë¡œ passkey retrieval taskë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼, LongLoRAê°€ ê¸°ì¡´ LoRAë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

![LongLoRA result 2](images/LongLoRA_passkey_acc.png)

> extended PI: ì¶”ê°€ ë¯¸ì„¸ì¡°ì • ì—†ì´ position interpolationìœ¼ë¡œ 48kê¹Œì§€ í™•ì¥í•œ ì„¤ì •

> **Notes**: **Passkey Retrieval Task**
>
> - ë§¤ìš° ê¸´ ë¬¸ì„œì— passkeyë¥¼ ì‚½ì…í•˜ê³ , ë§ˆì§€ë§‰ì— í•´ë‹¹ passkeyë¥¼ ì§ˆë¬¸í•œë‹¤.
>
> ![passkey retrieval example](images/passkey_retrieval_eg.png)

---

## 15.2 Evaluation of Long-Context LLMs

> í‰ê°€ì˜ ì–´ë ¤ì›€: ëª¨ë¸ì´ ìœ ì°½í•˜ê²Œ ê¸´ ì‘ë‹µì„ ìƒì„±í•œë‹¤ê³  í•´ì„œ, long-contextë¥¼ ì˜ ì´í•´í•œë‹¤ê³  ë³´ì¥í•  ìˆ˜ ì—†ë‹¤.

---

### 15.2.1 The Lost in the Middle Phenomenon

> [Lost in the Middle: How Language Models Use Long Contexts ë…¼ë¬¸(2023)](https://arxiv.org/abs/2307.03172)

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” long contextë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´, ë¬¸ë§¥ ë‚´ë¶€ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì‹ë³„í•´ì•¼ í•˜ëŠ” ë‘ ê°€ì§€ taskì—ì„œ LLMì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì˜€ë‹¤.

> multi-document question answering, key-value retrieval

ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ê°€ ë¬¸ì„œì˜ ì‹œì‘ê³¼ ëì— ìœ„ì¹˜í•˜ë©´ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ì§€ë§Œ, ì¤‘ê°„ì— ìœ„ì¹˜í•˜ë©´ ë‚®ì€ ì •í™•ë„ë¥¼ ë³´ì´ëŠ” í˜„ìƒì„ ê´€ì°°í•˜ì˜€ë‹¤. (**Lost in the Middle**)

![relevant information](images/relevant_information_position.png)

---

### 15.2.2 Needle In A Haystack Analysis

> [gkamradt github: Needle In A Haystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)

**needle in a haystack**ì€ LLMì´ long contextì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì°¾ëŠ”ì§€ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ì´ë‹¤. (in-context retrieval ability)

| Terminology | Description | Example |
| --- | --- | --- |
| **needle** | ì‚½ì…í•˜ëŠ” íŠ¹ì • ì •ë³´ | "eating a sandwich" |
| **haystack** | long context | "The best thing in San Francisco is eating a sandwich and sitting Dolores Park on a sunny day." |

> ì§ˆë¬¸: "What is the best thing to do in San Francisco?"

GPT-4 128K ëŒ€ìƒ ì‹¤í—˜ì—ì„œë„, ì•½ 25% depthë¶€í„°ëŠ” ì •í™•ë„ê°€ 0%ê¹Œì§€ ë–¨ì–´ì§€ëŠ” ì¸¡ì • ê²°ê³¼ë¥¼ ê´€ì°°í•  ìˆ˜ ìˆë‹¤. (ê¸´ context length ì„¤ì •ì¼ìˆ˜ë¡ ë‘ë“œëŸ¬ì§„ë‹¤.)

![needle in a haystack](images/GPT_4_testing.png)

> x: context length, y: depth, ìƒ‰ìƒ: retrieval ì •í™•ë„

---

### 15.2.3 LongBench

> [LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding ë…¼ë¬¸(2023)](https://arxiv.org/abs/2308.14508)

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” long-contextë¥¼ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ë¡œ **LongBench**ë¥¼ ì†Œê°œí•˜ì˜€ë‹¤.

- 6ê°œ task(e.g., QA, summarization), 21ê°œ dataset í¬í•¨ (ì˜ì–´, ì¤‘êµ­ì–´ ì œê³µ)

- ìµœëŒ€ 13,000+ token ì§€ì›

![LongBench](images/LongBench_num_data.png)

---

## 15.3 Challenges of Deploying Long-Context LLMs

---

### 15.3.1 Recap: KV Cache

long contextë¥¼ ë‹¤ë£¨ëŠ” LLMì€ ê·¸ë§Œí¼ í° KV cache sizeë¥¼ í•„ìš”ë¡œ í•œë‹¤.

- Llama-2-70B, KV cache size

$$ \underset{minibatch}{BS} * \underset{layers}{80} * \underset{heads}{64} * \underset{n_{emd} }{128} * \underset{length}{N} * \underset{K,V}{2} * {2}\mathrm{bytes} = 2.5\mathrm{MB} \times BS \times N $$

> bs=1, n_seq=512: 1.25GB

> bs=16, n_seq=4096: 160GB (= A100 x2)

ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ë”°ë¼ì„œëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆë³´ë‹¤ë„ í° KV cacheê°€ í•„ìš”í•  ìˆ˜ ìˆë‹¤.

![KV cache example](images/KV_cache_example.png)

---

### 15.3.2 Extensive VRAM Memory Usage

> [Tom Aarsen github: Attention Sinks in Transformers for endless fluent generation](https://github.com/tomaarsen/attention_sinks)

ë‹¤ìŒì€ 3ê°€ì§€ attention ë°©ì‹ì—ì„œ, ì…ë ¥ ê¸¸ì´ì— ë”°ë¥¸ perplexity(ì‹¤ì„ )ì™€ VRAM ì‚¬ìš©ëŸ‰(ì ì„ )ì„ ë¹„êµí•œ ë„í‘œì´ë‹¤.
 (Llama2-7B ëª¨ë¸)

| Attention | VRAM | Perplexity |
| --- | --- | --- |
| **transformer**(ì´ˆë¡ìƒ‰) | ì„ í˜• ì¦ê°€ | ì‚¬ì „í•™ìŠµ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ì„±ëŠ¥ ì €í•˜(4K~) |
| **windowed**(ì£¼í™©ìƒ‰) | 1024ê°œ í† í° ìœ ì§€ | ì²« í† í°ì´ windowë¥¼ ë²—ì–´ë‚˜ë©´(evicted) ì„±ëŠ¥ ì €í•˜ |

![attention sink](images/attention_sink_llama-2.png)

---

## 15.4 StreamingLLM

> [Efficient Streaming Language Models with Attention Sinks ë…¼ë¬¸(2023)](https://arxiv.org/abs/2309.17453)

streaming application í™˜ê²½ì—ì„œëŠ” ëŠì„ì—†ì´ ì±—ë´‡ê³¼ ëŒ€í™”í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. ê·¸ëŸ¬ë‚˜, LLMì˜ decoding ê³¼ì •ì—ì„œëŠ” êµ‰ì¥íˆ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œ í•œë‹¤.

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ë¥¼ ë°©ì§€í•˜ë©´ì„œ OOMì„ ë°©ì§€í•˜ëŠ” í•´ê²°ì±…ìœ¼ë¡œ, StreamingLLM í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì˜€ë‹¤.

---

### 15.4.1 The Limits of Window Attention

OOMì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ, window attentionì²˜ëŸ¼ local tokenë§Œ ìºì‹±í•˜ëŠ” ì—°ì‚°ì„ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤.

- (í…ìŠ¤íŠ¸ ê¸¸ì´ > ìºì‹œ í¬ê¸°) ê·¸ëŸ¬ë‚˜ ì²« í† í°ì´ ìœˆë„ìš°ë¥¼ ë²—ì–´ë‚˜ëŠ” ìˆœê°„, ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ëœë‹¤.

| Window Attention | Perplexity |
| :---: | :---: |
| ![window](images/StreamingLLM_vs_existing_2.png) |  ![log PPL](images/streamingllm_log_ppl.png) |

> ì£¼í™©ìƒ‰: window attention, íŒŒë€ìƒ‰: dense attention

> | Dense | Window | Sliding Window<br>w/ Re-computation | 
> | :---: | :---: | :---: |
> | ![dense](images/StreamingLLM_vs_existing_1.png) | ![window](images/StreamingLLM_vs_existing_2.png) | ![sliding window](images/StreamingLLM_vs_existing_3.png) |
> | $O(T^2)$ âœ˜ | $O(TL)$ âœ” | $O(TL^2)$ âœ˜ |
> | **PPL**: 5641 âœ˜ | **PPL**: 5158 âœ˜ | **PPL**: 5.43 âœ” |

---

### 15.4.2 Attention Sink Phenomenon

ë˜í•œ, ë…¼ë¬¸ì—ì„œëŠ” ì²« ë²ˆì§¸ í† í°ì— heavy attentionì´ ì§‘ì¤‘ë˜ëŠ” **attention sink** í˜„ìƒì„ ê´€ì°°í•˜ì˜€ë‹¤.

![attention sink](images/attention_sink.png)

> Llama2-7B, 256 sentences(length 16)

ì°¸ê³ ë¡œ ì´ì „ SpAtten ë…¼ë¬¸ì—ì„œë„ ìœ ì‚¬í•œ í˜„ìƒì„ ê´€ì°°í•˜ì˜€ëŠ”ë°, ë‹¹ì‹œì—ë„ ì²« ë²ˆì§¸ í† í°ë§Œì€ pruneë˜ì§€ ì•Šì•˜ë‹¤.

![SpAtten attention sink](images/SpAtten_first_token.png)

ë…¼ë¬¸ì—ì„œëŠ” attention sink í˜„ìƒì˜ ì›ì¸ìœ¼ë¡œ softmaxë¥¼ ì§€ëª©í•œë‹¤. softmaxëŠ” ì´í•©ì´ 1ì´ ë˜ì–´ì•¼ í•˜ëŠ”ë°, ì´ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ í† í°ì˜ scoreê°€ ê³¼ë„í•˜ê²Œ ì»¤ì§€ê²Œ ëœë‹¤.

$$ \mathrm{Softmax} (x)_i = \frac{e^{x_i}}{e^{x_1} + \sum_{j=2}^{N}e^{x_j}}, \quad x_1 \gg x_j, j \in 2, \cdots, N $$

ì²« ë²ˆì§¸ í† í°ì€ ì´ì–´ì§€ëŠ” í† í°ì—ì„œ í•­ìƒ ê´€ì°°í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ(autoregressive language modeling), í¸í–¥ì´ ë°œìƒí•˜ëŠ” ì§€ì ì´ ëœë‹¤.

![attention sink example](images/attention_sink_example.png)

---

#### 15.4.2.1 Position vs. Semantics

ê·¸ë ‡ë‹¤ë©´ ì²« ë²ˆì§¸ í† í°ì— í¸í–¥ì´ ì¼ì–´ë‚˜ëŠ” ì´ìœ ëŠ” ë‹¨ìˆœ position ë•Œë¬¸ì¼ê¹Œ, ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ í† í°ì´ ê°–ëŠ” semantics ë•Œë¬¸ì¼ê¹Œ?

ë‹¤ìŒì€ Llama-2-13B ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ìœ¼ë¡œ, í† í° ì•ì— `"\n"` 4ê°œë¥¼ ì‚½ì…í•˜ëŠ” ê²ƒìœ¼ë¡œ perplexityê°€ íšŒë³µëœ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.

![attention sink rationale](images/attention_sink_rationale.png)

ì¦‰, ì´ëŸ¬í•œ í¸í–¥ì€ **position** ë•Œë¬¸ì´ë‹¤.

---

### 15.4.3 StreamingLLM Framework

ì´ëŸ¬í•œ ê´€ì°°ì„ ë°”íƒ•ìœ¼ë¡œ, StreamingLLMì—ì„œëŠ” KV cacheì— í•­ìƒ attention sink token(ì²« ë²ˆì§¸ í† í°)ì„ ìœ ì§€í•œë‹¤.

| Attention | KV Cache |
| :---: | :---: |
| ![streaming window](images/StreamingLLM_vs_existing_4.png) | ![streamingLLM KV cache](images/StreamingLLM_KV.png) |

êµ¬í˜„ì—ì„œëŠ” í† í°ì˜ í…ìŠ¤íŠ¸ ë‚´ ì›ë˜ ìœ„ì¹˜ê°€ ì•„ë‹Œ, ìºì‹œ ë‚´ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ relative distanceì™€ positional informationì„ ê³„ì‚°í•œë‹¤.

![streamingLLM position in cache](images/streamingllm_position_in_cache.png)

---

### 15.4.4 Streaming Performance

ê¸°ì¡´ 3ê°€ì§€ attention ë°©ì‹ê³¼ ë¹„êµí•œ ì‹¤í—˜ì—ì„œ, StreamingLLM(ë¹¨ê°„ìƒ‰)ì´ ê°€ì¥ ìš°ìˆ˜í•œ perplexityë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.

![streamingLLM performance 1](images/streamingllm_perf_1.png)

ë‹¤ìŒì€ ìµœëŒ€ 4M ê¸¸ì´ì— ë‹¬í•˜ëŠ” long-contextì—ì„œì˜ ì„±ëŠ¥ì„ ë¹„êµí•œ ê²°ê³¼ë‹¤.

![streamingLLM performance 2](images/streamingllm_perf_2.png)

---

### 15.4.5 Efficiency

sliding window w. re-computationì€ ìœˆë„ìš° ë‚´ë¶€ì—ì„œ quadratic attention ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤. ì´ì™€ ë¹„êµí–ˆì„ ë•Œ, StreamingLLMì€ ìµœëŒ€ 22.2x ì§€ì—° ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆì—ˆë‹¤.

![streamingLLM efficiency](images/StreamingLLM_efficiency.png)

---

### 15.4.6 Ablation Study: \#Attention Sinks

ì¶”ê°€ë¡œ ë…¼ë¬¸ì—ì„œëŠ”, ìœ ì§€í•´ì•¼ í•˜ëŠ” attention sinkì˜ ê°œìˆ˜ë¥¼ ablation studyì—ì„œ ê²€ì¦í•˜ì˜€ë‹¤.

![streamingLLM ablation 1](images/streamingllm_ablation_1.png)

---

### 15.4.7 Pre-training with a Dedicated Attention Sink

ë°˜ëŒ€ë¡œ, ì˜¤ì§ í•˜ë‚˜ì˜ attention sink í† í°ë§Œ í•„ìš”í•˜ë„ë¡ LLMì„ í•™ìŠµí•  ìˆ˜ë„ ìˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë“  í•™ìŠµ ìƒ˜í”Œì—ì„œ, ì²« ë²ˆì§¸ í† í°ìœ¼ë¡œ extra learning tokenì„ ì‚½ì…í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤.

> **Notes**: sink tokenì„ ì¶”ê°€(ì£¼í™©ìƒ‰)í•´ë„ ìœ ì‚¬í•œ loss ê³¡ì„ ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤.
>
> ![streamingLLM ablation 2](images/streamingllm_ablation_2.png)

ì‹¤ì œë¡œ, í•´ë‹¹ í† í°ì„ ì¶”ê°€í•œ ëª¨ë¸ì—ì„œëŠ”í•˜ë‚˜ì˜ attention sinkë§Œ ìœ ì§€í•´ë„ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ì§€ ì•Šì•˜ë‹¤.

![streamingLLM ablation 3](images/streamingllm_ablation_3.png)

---